{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Il faut se mettre dans un environnement ayant les conditions requises pour faire tourner imageai (i.e. Python 3.7.6)\n",
    "\n",
    "from imageai.Detection.Custom import DetectionModelTrainer, CustomObjectDetection\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.getcwd()\n",
    "model_type = \"retinanet\"\n",
    "dataset_directory = os.path.join(current_path, \"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating anchor boxes for training images and annotation...\n",
      "Average IOU for 9 anchors: 0.75\n",
      "Anchor Boxes generated.\n",
      "Detection configuration saved in  c:\\Users\\geyma\\Documents\\Centrale Digital Lab\\Projet Auchan\\auchan_cdl\\dataset\\json\\detection_config.json\n",
      "Evaluating over 21 samples taken from c:\\Users\\geyma\\Documents\\Centrale Digital Lab\\Projet Auchan\\auchan_cdl\\dataset\\validation\n",
      "Training over 78 samples  given at c:\\Users\\geyma\\Documents\\Centrale Digital Lab\\Projet Auchan\\auchan_cdl\\dataset\\train\n",
      "Training on: \t['Logo AB', 'Logo BIO', 'Logo EU']\n",
      "Training with Batch Size:  4\n",
      "Number of Training Samples:  78\n",
      "Number of Validation Samples:  21\n",
      "Number of Experiments:  10\n",
      "Pre-trained Model not provided. Transfer learning not in use.\n",
      "Training will start with 3 warmup experiments\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer YoloLayer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/13\n",
      "160/160 [==============================] - 1680s 10s/step - loss: 320.9408 - yolo_layer_3_loss: 39.4942 - yolo_layer_4_loss: 81.7157 - yolo_layer_5_loss: 188.1526 - val_loss: 93.7822 - val_yolo_layer_3_loss: 9.3446 - val_yolo_layer_4_loss: 17.4602 - val_yolo_layer_5_loss: 55.4469\n",
      "Epoch 2/13\n",
      "160/160 [==============================] - 1551s 10s/step - loss: 58.9218 - yolo_layer_3_loss: 6.3342 - yolo_layer_4_loss: 12.2607 - yolo_layer_5_loss: 28.8601 - val_loss: 55.3468 - val_yolo_layer_3_loss: 6.6969 - val_yolo_layer_4_loss: 10.6500 - val_yolo_layer_5_loss: 26.8212\n",
      "Epoch 3/13\n",
      "160/160 [==============================] - 1191s 7s/step - loss: 45.9389 - yolo_layer_3_loss: 5.4197 - yolo_layer_4_loss: 10.7923 - yolo_layer_5_loss: 18.6823 - val_loss: 42.6889 - val_yolo_layer_3_loss: 5.9558 - val_yolo_layer_4_loss: 9.6763 - val_yolo_layer_5_loss: 16.4796\n",
      "Epoch 4/13\n",
      "160/160 [==============================] - 1316s 8s/step - loss: 40.1362 - yolo_layer_3_loss: 4.8821 - yolo_layer_4_loss: 9.9256 - yolo_layer_5_loss: 14.9162 - val_loss: 128.9762 - val_yolo_layer_3_loss: 74.4579 - val_yolo_layer_4_loss: 14.8466 - val_yolo_layer_5_loss: 29.8073\n",
      "Epoch 5/13\n",
      "160/160 [==============================] - 1541s 10s/step - loss: 37.4830 - yolo_layer_3_loss: 4.8689 - yolo_layer_4_loss: 9.6351 - yolo_layer_5_loss: 13.3079 - val_loss: 56.8858 - val_yolo_layer_3_loss: 6.1829 - val_yolo_layer_4_loss: 11.5107 - val_yolo_layer_5_loss: 30.0827\n",
      "Epoch 6/13\n",
      "160/160 [==============================] - 1355s 8s/step - loss: 36.0664 - yolo_layer_3_loss: 4.8247 - yolo_layer_4_loss: 9.4366 - yolo_layer_5_loss: 12.8675 - val_loss: 100.4073 - val_yolo_layer_3_loss: 37.5496 - val_yolo_layer_4_loss: 13.2582 - val_yolo_layer_5_loss: 41.1937\n",
      "Epoch 7/13\n",
      "160/160 [==============================] - 1405s 9s/step - loss: 34.1868 - yolo_layer_3_loss: 3.9936 - yolo_layer_4_loss: 9.1683 - yolo_layer_5_loss: 12.7855 - val_loss: 35.3654 - val_yolo_layer_3_loss: 5.0218 - val_yolo_layer_4_loss: 9.0368 - val_yolo_layer_5_loss: 13.5309\n",
      "Epoch 8/13\n",
      "160/160 [==============================] - 1357s 8s/step - loss: 32.7277 - yolo_layer_3_loss: 3.9032 - yolo_layer_4_loss: 8.5933 - yolo_layer_5_loss: 12.5706 - val_loss: 60.7502 - val_yolo_layer_3_loss: 25.6544 - val_yolo_layer_4_loss: 12.2845 - val_yolo_layer_5_loss: 15.5112\n",
      "Epoch 9/13\n",
      "160/160 [==============================] - 1407s 9s/step - loss: 31.9438 - yolo_layer_3_loss: 4.5110 - yolo_layer_4_loss: 8.6950 - yolo_layer_5_loss: 11.5557 - val_loss: 35.6757 - val_yolo_layer_3_loss: 5.0563 - val_yolo_layer_4_loss: 9.8448 - val_yolo_layer_5_loss: 13.9288\n",
      "Epoch 10/13\n",
      "160/160 [==============================] - 1272s 8s/step - loss: 29.7186 - yolo_layer_3_loss: 3.3386 - yolo_layer_4_loss: 8.1877 - yolo_layer_5_loss: 11.4325 - val_loss: 41.2717 - val_yolo_layer_3_loss: 9.8665 - val_yolo_layer_4_loss: 9.6295 - val_yolo_layer_5_loss: 15.2530\n",
      "Epoch 11/13\n",
      "160/160 [==============================] - 1416s 9s/step - loss: 30.1962 - yolo_layer_3_loss: 4.4737 - yolo_layer_4_loss: 8.5220 - yolo_layer_5_loss: 10.7560 - val_loss: 37.2346 - val_yolo_layer_3_loss: 4.1143 - val_yolo_layer_4_loss: 9.4571 - val_yolo_layer_5_loss: 17.4296\n",
      "Epoch 12/13\n",
      "160/160 [==============================] - 1566s 10s/step - loss: 29.8627 - yolo_layer_3_loss: 5.1623 - yolo_layer_4_loss: 8.5071 - yolo_layer_5_loss: 10.0095 - val_loss: 87.8937 - val_yolo_layer_3_loss: 13.8498 - val_yolo_layer_4_loss: 13.2695 - val_yolo_layer_5_loss: 54.7323\n",
      "Epoch 13/13\n",
      "160/160 [==============================] - 1474s 9s/step - loss: 28.5519 - yolo_layer_3_loss: 4.1296 - yolo_layer_4_loss: 8.1697 - yolo_layer_5_loss: 10.2614 - val_loss: 31.4568 - val_yolo_layer_3_loss: 4.7685 - val_yolo_layer_4_loss: 8.9748 - val_yolo_layer_5_loss: 11.8491\n"
     ]
    }
   ],
   "source": [
    "trainer = DetectionModelTrainer()\n",
    "trainer.setModelTypeAsYOLOv3()\n",
    "trainer.setDataDirectory(data_directory = dataset_directory)\n",
    "trainer.setTrainConfig(object_names_array = [\"Logo AB\", \"Logo EU\", \"Logo BIO\"], batch_size=4, num_experiments=10)  # We can train from pretrained model here\n",
    "trainer.trainModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Model evaluation....\n",
      "Evaluating over 21 samples taken from c:\\Users\\geyma\\Documents\\Centrale Digital Lab\\Projet Auchan\\auchan_cdl\\dataset\\validation\n",
      "Training over 78 samples  given at c:\\Users\\geyma\\Documents\\Centrale Digital Lab\\Projet Auchan\\auchan_cdl\\dataset\\train\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model File:  dataset/models/detection_model-ex-013--loss-0028.823.h5 \n",
      "\n",
      "Evaluation samples:  21\n",
      "Using IoU:  0.5\n",
      "Using Object Threshold:  0.2\n",
      "Using Non-Maximum Suppression:  0.45\n",
      "Logo AB: 0.0407\n",
      "Logo BIO: 0.0417\n",
      "Logo EU: 0.0000\n",
      "mAP: 0.0275\n",
      "===============================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'model_file': 'dataset/models/detection_model-ex-013--loss-0028.823.h5',\n",
       "  'using_iou': 0.5,\n",
       "  'using_object_threshold': 0.2,\n",
       "  'using_non_maximum_suppression': 0.45,\n",
       "  'average_precision': {'Logo AB': 0.040705484071562895,\n",
       "   'Logo BIO': 0.041666666666666664,\n",
       "   'Logo EU': 0.0},\n",
       "  'evaluation_samples': 21,\n",
       "  'map': 0.027457383579409855}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluateModel(model_path=\"dataset/models/detection_model-ex-013--loss-0028.823.h5\", json_path=\"dataset/json/detection_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "output_image_path must be the path where to write the image. Therefore it must end as one the following: '.jpg', '.png', '.tif', '.webp', '.PPM', '.PGM'.  found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15952/2795579972.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetJsonPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dataset/json/detection_config.json\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectObjectsFromImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_image\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"dataset/img_test.jpeg\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput_image_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\Centrale Digital Lab\\Projet Auchan\\venv_Auchan\\lib\\site-packages\\imageai\\Detection\\Custom\\__init__.py\u001b[0m in \u001b[0;36mdetectObjectsFromImage\u001b[1;34m(self, input_image, output_image_path, input_type, output_type, extract_detected_objects, minimum_percentage_probability, nms_treshold, display_percentage_probability, display_object_name, thread_safe)\u001b[0m\n\u001b[0;32m    754\u001b[0m                     raise ValueError(\"output_image_path must be the path where to write the image. \"\n\u001b[0;32m    755\u001b[0m                                      \u001b[1;34m\"Therefore it must end as one the following: \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 756\u001b[1;33m                                      \"'.jpg', '.png', '.tif', '.webp', '.PPM', '.PGM'. {} found\".format(output_image_path))\n\u001b[0m\u001b[0;32m    757\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mextract_detected_objects\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m                     \u001b[1;31m# Results must be written as files and need to extract detected objects as images,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: output_image_path must be the path where to write the image. Therefore it must end as one the following: '.jpg', '.png', '.tif', '.webp', '.PPM', '.PGM'.  found"
     ]
    }
   ],
   "source": [
    "detector = CustomObjectDetection()\n",
    "detector.setModelTypeAsYOLOv3()\n",
    "detector.setModelPath(\"dataset/models/detection_model-ex-013--loss-0028.823.h5\")\n",
    "detector.setJsonPath(\"dataset/json/detection_config.json\")\n",
    "detector.loadModel()\n",
    "detector.detectObjectsFromImage(input_image=\"dataset/img_test.jpeg\",output_image_path=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various sources for preprocessing :\n",
    "https://prince-canuma.medium.com/image-pre-processing-c1aec0be3edf\n",
    "https://www.geeksforgeeks.org/python-intensity-transformation-operations-on-images/\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "97999f551398375d64993be89bf124a1ccff502cde73f9823aa9d07aa4f3ccfc"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
