{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 21,
=======
<<<<<<< Updated upstream
   "execution_count": 5,
=======
   "execution_count": 1,
>>>>>>> Stashed changes
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Entrainement.CamemBERT.camemBERT import text_prepare\n",
    "import shutil, os"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 22,
=======
<<<<<<< Updated upstream
   "execution_count": 4,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7608/364973396.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_result['label'] = label\n"
     ]
    }
   ],
=======
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
>>>>>>> Stashed changes
   "source": [
    "path = '/home/jeremy/AUCHAN 2/IMAGES/4992 images triees/avec les labels corriges'\n",
    "path_img_test = '/home/jeremy/Documents/GitHub/auchan_cdl/Entrainement/CamemBERT/Data/img_val.csv'\n",
    "path_descr_test = '/home/jeremy/Documents/GitHub/auchan_cdl/Entrainement/CamemBERT/Data/val.csv'\n",
    "path_data = '/home/jeremy/AUCHAN 2/auchan_cdl/Training/CamemBERT/Data/text_detector_data.csv'\n",
    "df_test = pd.read_csv(path_descr_test, index_col=False)\n",
    "df_img_test = pd.read_csv(path_img_test, index_col=False)\n",
    "data = pd.read_csv(path_data, index_col=False)\n",
    "data.dropna(inplace=True)\n",
    "description = [text_prepare(data['description_fournisseur'].iloc[i]+ ' ' + data['description_produit'].iloc[i]) for i in range(len(data))]\n",
    "df = df_test.copy()\n",
    "df['image'] = df_img_test.image\n",
    "del df['Unnamed: 0']\n",
    "df_result=df[['image','text']]\n",
    "df_result.rename(columns={'text': 'description'}, inplace=True)\n",
    "label = []\n",
    "columns = df.columns\n",
    "\n",
    "for i in range(len(df)):\n",
    "    label.append(columns[df.iloc[i].values.tolist().index(1.0)])\n",
    "df_result['label'] = label\n",
    "for i in range(len(df_result)):\n",
    "    df_result.image.iloc[i] = data.image.iloc[description.index(df_result.description.iloc[i])]\n",
    "\n",
    "df_result.to_csv(os.path.join('/home/jeremy/Documents/GitHub/auchan_cdl/Data/Predictions_classification','prediction.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_result)):\n",
    "    shutil.copyfile(os.path.join(path,df_result.label.iloc[i],df_result.image.iloc[i]), os.path.join('/home/jeremy/Documents/GitHub/auchan_cdl/Data/Predictions_classification',df_result.image.iloc[i]))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
<<<<<<< Updated upstream
   "execution_count": 6,
=======
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import dirname, abspath\n",
    "import sys, os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from fast_bert.data_cls import BertDataBunch\n",
    "from fast_bert.data_lm import BertLMDataBunch\n",
    "from fast_bert.learner_lm import BertLMLearner\n",
    "from fast_bert.learner_cls import BertLearner\n",
    "from fast_bert.metrics import accuracy, Exact_Match_Ratio\n",
    "import logging\n",
    "import unidecode\n",
    "from pathlib import Path\n",
    "root_path = dirname(abspath('text_classification.py'))\n",
    "sys.path.append(root_path)\n",
    "import shutil\n",
    "from config import CFG\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "final_stopwords_list = nltk.corpus.stopwords.words('english') + nltk.corpus.stopwords.words('french')\n",
    "\n",
    "\n",
    "\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "NEW_LINE = re.compile('\\n')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(final_stopwords_list)\n",
    "\n",
    "def text_prepare(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = text.lower()# lowercase text\n",
    "    text = unidecode.unidecode(text)\n",
    "    text = NEW_LINE.sub(' ',text) # replace NEW_LINE symbols in our texts by space\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ',text)# replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('',text)# delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = ' '.join([word for word in text.split() if word not in STOPWORDS])\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocessing(data):\n",
    "    data.dropna(inplace=True)\n",
    "    labels = data.label.unique().tolist()\n",
    "    X1 = data['description_fournisseur'].tolist()\n",
    "    X2 = data['description_produit'].tolist()\n",
    "    X = [X1[i] +' '+ X2[i] for i in range(len(X1))]\n",
    "    dico = {label:i for i, label in enumerate(labels)}\n",
    "    imgs = data.index.tolist()\n",
    "    Y = data.label.tolist()\n",
    "    Y = [dico[label] for label in Y]\n",
    "    Y = tf.keras.utils.to_categorical(Y, num_classes=len(dico))\n",
    "    Y = np.array(Y)\n",
    "    X = [text_prepare(text) for text in X]\n",
    "    X = np.array(X)\n",
    "\n",
    "    x, x_test, y, y_test, img, img_test = train_test_split(X,Y,imgs,test_size=0.05,train_size=0.95)\n",
    "    x_train, x_val, y_train, y_val, img_train, img_val = train_test_split(x,y,img,test_size = 0.15,train_size =0.85)\n",
    "\n",
    "    df_lab = pd.DataFrame(columns=['label'])\n",
    "    df_lab.label = labels\n",
    "\n",
    "    df_train = pd.DataFrame(columns = ['text']+labels)\n",
    "    df_val = pd.DataFrame(columns = ['text']+labels)\n",
    "    df_test = pd.DataFrame(columns = ['text']+labels)\n",
    "\n",
    "    df_img_train = pd.DataFrame(columns=['image'])\n",
    "    df_img_val = pd.DataFrame(columns=['image'])\n",
    "    df_img_test = pd.DataFrame(columns=['image'])\n",
    "\n",
    "    df_train['text'] = x_train\n",
    "    df_train[labels] = y_train\n",
    "\n",
    "    df_val['text'] = x_val\n",
    "    df_val[labels] = y_val\n",
    "\n",
    "    df_test['text'] = x_test\n",
    "    df_test[labels] = y_test\n",
    "\n",
    "    df_img_train.image = img_train\n",
    "    df_img_test.image = img_test\n",
    "    df_img_val.image = img_val\n",
    "    df_img_train.to_csv(os.path.join(CFG.path_bert, 'Data/img_train.csv'))\n",
    "    df_img_val.to_csv(os.path.join(CFG.path_bert, 'Data/img_val.csv'))\n",
    "    df_img_test.to_csv(os.path.join(CFG.path_bert, 'Data/img_test.csv'))\n",
    "    df_train.to_csv(os.path.join(CFG.path_bert, 'Data/train.csv'))\n",
    "    df_val.to_csv(os.path.join(CFG.path_bert,'Data/val.csv'))\n",
    "    df_test.to_csv(os.path.join(CFG.path_bert,'Data/test.csv'))\n",
    "    df_lab.to_csv(os.path.join(CFG.path_bert,'Data/labels.csv'), header=False, index=False)\n",
    "    return X\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def model(tuning):\n",
    "    DATA_PATH = Path(os.path.join(CFG.path_bert,'Data/'))\n",
    "\n",
    "     # texts = preprocessing(df)\n",
    "    x_test = pd.read_csv('Entrainement/CamemBERT/Data/test.csv', index_col=False)\n",
    "    x_train = pd.read_csv('Entrainement/CamemBERT/Data/train.csv', index_col=False)\n",
    "    x_val = pd.read_csv('Entrainement/CamemBERT/Data/val.csv', index_col=False)\n",
    "    texts = x_train.text.tolist() + x_test.text.tolist() + x_val.text.tolist()\n",
    "\n",
    "    if tuning:\n",
    "        version_fine_tuned = len(os.listdir(os.path.join(CFG.path_models, 'CamemBERT_fine_tuned')))+1\n",
    "        os.mkdir(os.path.join(CFG.path_models, 'CamemBERT_fine_tuned','CamemBERT_fine_tuned_v{}'.format(version_fine_tuned)))\n",
    "        os.mkdir(os.path.join(CFG.path,'Tensorboard', 'CamemBERT_fine_tuned','CamemBERT_fine_tuned_v{}'.format(version_fine_tuned)))\n",
    "        os.mkdir(os.path.join(CFG.path,'Tensorboard', 'CamemBERT_fine_tuned','CamemBERT_fine_tuned_v{}'.format(version_fine_tuned),'tensorboard'))\n",
    "        OUTPUT_DIR = Path(CFG.path,'Tensorboard','CamemBERT_fine_tuned','CamemBERT_fine_tuned_v{}'.format(version_fine_tuned))\n",
    "        WGTS_PATH = Path(CFG.path_models,'CamemBERT_fine_tuned','CamemBERT_fine_tuned_v{}'.format(version_fine_tuned) ,'pytorch_model.bin')\n",
    "        MODEL_PATH = Path(os.path.join(CFG.path_models, 'CamemBERT_fine_tuned','CamemBERT_fine_tuned_v{}'.format(version_fine_tuned)))\n",
    "        logger = logging.getLogger()\n",
    "        databunch_lm = BertLMDataBunch.from_raw_corpus(\n",
    "                        data_dir=DATA_PATH,\n",
    "                        text_list=texts,\n",
    "                        tokenizer='camembert-base',\n",
    "                        batch_size_per_gpu=4,\n",
    "                        max_seq_length=512,\n",
    "                        multi_gpu=False,\n",
    "                        model_type='camembert-base',\n",
    "                        logger=logger)\n",
    "        lm_learner = BertLMLearner.from_pretrained_model(\n",
    "                                dataBunch=databunch_lm,\n",
    "                                pretrained_path='camembert-base',\n",
    "                                output_dir=OUTPUT_DIR,\n",
    "                                metrics=[],\n",
    "                                device=torch.device(CFG.device),\n",
    "                                logger=logger,\n",
    "                                multi_gpu=False,\n",
    "                                logging_steps=50,\n",
    "                                fp16_opt_level=\"O2\")\n",
    "        lm_learner.fit(epochs=30,\n",
    "                lr=1e-4,\n",
    "                validate=True,\n",
    "                schedule_type=\"warmup_cosine\",\n",
    "                optimizer_type=\"adamw\")\n",
    "        lm_learner.validate()  \n",
    "        lm_learner.save_model(MODEL_PATH)  \n",
    "    else:\n",
    "        version_fine_tuned = len(os.listdir(os.path.join(CFG.path_models, 'CamemBERT_fine_tuned')))\n",
    "        WGTS_PATH = Path(CFG.path_models,'CamemBERT_fine_tuned','CamemBERT_fine_tuned_v{}'.format(version_fine_tuned) ,'pytorch_model.bin')\n",
    "        MODEL_PATH = Path(os.path.join(CFG.path_models, 'CamemBERT_fine_tuned','CamemBERT_fine_tuned_v{}'.format(version_fine_tuned)))\n",
    "\n",
    "\n",
    "    version_camembert = len(os.listdir(os.path.join(CFG.path_models, 'CamemBERT')))+1\n",
    "    os.mkdir(os.path.join(CFG.path_models, 'CamemBERT','CamemBERT_v{}'.format(version_camembert)))\n",
    "    os.mkdir(os.path.join(CFG.path,'Tensorboard', 'CamemBERT','CamemBERT_v{}'.format(version_camembert)))\n",
    "    BERT_PATH = Path(os.path.join(CFG.path_models, 'CamemBERT','CamemBERT_v{}'.format(version_camembert)))\n",
    "\n",
    "    try:\n",
    "        filename = os.listdir(os.path.join(CFG.path_data,'Entrainement_camemBERT'))[0]\n",
    "    except:\n",
    "        print(\"Aucun csv d'entrainement\")\n",
    "        pass\n",
    "    df = pd.read_csv(os.path.join(CFG.path_data,'Entrainement_camemBERT',filename))\n",
    "   \n",
    "    # os.remove(os.path.join(CFG.path_data,'Entrainement_camemBERT',filename))\n",
    "        \n",
    "\n",
    "    OUTPUT_DIR_BERT = Path(os.path.join(CFG.path,'Tensorboard', 'CamemBERT','CamemBERT_v{}'.format(version_camembert)))\n",
    "    labels = pd.read_csv(os.path.join(DATA_PATH,'labels.csv'), header=None, index_col=False)[0].tolist()\n",
    "    databunch = BertDataBunch(DATA_PATH, DATA_PATH,\n",
    "                          tokenizer='camembert-base',\n",
    "                          train_file='train.csv',   \n",
    "                          val_file='val.csv',\n",
    "                          label_file='labels.csv',\n",
    "                          text_col='text',\n",
    "                          label_col=labels,\n",
    "                          batch_size_per_gpu=4,\n",
    "                          max_seq_length=512,\n",
    "                          multi_gpu=False,\n",
    "                          multi_label=True,\n",
    "                          model_type='camembert-base')\n",
    "    logger = logging.getLogger()\n",
    "    device_cuda = torch.device(\"cuda\")\n",
    "\n",
    "    metrics = [{'name': 'Exact_Match_Ratio', 'function': Exact_Match_Ratio}]\n",
    "    learner = BertLearner.from_pretrained_model(\n",
    "\t\t\t\t\t\tdatabunch,\n",
    "\t\t\t\t\t\tpretrained_path=MODEL_PATH,\n",
    "\t\t\t\t\t\tmetrics=metrics,\n",
    "\t\t\t\t\t\tdevice=device_cuda,\n",
    "\t\t\t\t\t\tlogger=logger,\n",
    "\t\t\t\t\t\toutput_dir=OUTPUT_DIR_BERT,\n",
    "\t\t\t\t\t\tfinetuned_wgts_path=WGTS_PATH,\n",
    "\t\t\t\t\t\twarmup_steps=500,\n",
    "\t\t\t\t\t\tmulti_gpu=False,\n",
    "\t\t\t\t\t\tis_fp16=True,\n",
    "\t\t\t\t\t\tmulti_label=True,\n",
    "\t\t\t\t\t\tlogging_steps=50)\n",
    "        \n",
    "    return learner, BERT_PATH\n",
    "\n",
    "def checkpoint(n_save, n_epochs, learner, BERT_PATH):\n",
    "    for epoch in range(int(n_epochs/n_save)):\n",
    "        learner.fit(epochs=n_save,\n",
    "                lr=9e-5,\n",
    "                validate=True, \t# Evaluate the model after each epoch\n",
    "                schedule_type=\"warmup_cosine\",\n",
    "                optimizer_type=\"adamw\")\n",
    "\n",
    "        print(learner.validate())\n",
    "        learner.save_model(Path(os.path.join(BERT_PATH, \"model_{}\".format(epoch))))\n",
    "\n",
    "def main_training_BERT(tuning):\n",
    "    files = os.listdir(os.path.join(CFG.path_bert,'Data'))\n",
    "    if 'cache' in files:\n",
    "        shutil.rmtree(os.path.join(CFG.path_bert,'Data','cache'))\n",
    "    learner, BERT_PATH = model(tuning)\n",
    "    checkpoint(1, 2, learner, BERT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/jeremy/Documents/GitHub/auchan_cdl/Models/CamemBERT_fine_tuned/CamemBERT_fine_tuned_v2 were not used when initializing CamembertForMultiLabelSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing CamembertForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForMultiLabelSequenceClassification were not initialized from the model checkpoint at /home/jeremy/Documents/GitHub/auchan_cdl/Models/CamemBERT_fine_tuned/CamemBERT_fine_tuned_v2 and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "learner, BERT_PATH = model(tuning=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "liste = learner.fit(epochs=1,\n",
    "            lr=9e-5,\n",
    "            validate=False, \t# Evaluate the model after each epoch\n",
    "            schedule_type=\"warmup_cosine\",\n",
    "            optimizer_type=\"adamw\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 229/894 [00:04<00:14, 46.37it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_185303/1852282104.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AUCHAN 2/pytorch_awesome/pytorch-awesome/lib/python3.8/site-packages/fast_bert/prediction.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AUCHAN 2/pytorch_awesome/pytorch-awesome/lib/python3.8/site-packages/fast_bert/prediction.py\u001b[0m in \u001b[0;36mpredict_batch\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AUCHAN 2/pytorch_awesome/pytorch-awesome/lib/python3.8/site-packages/fast_bert/learner_cls.py\u001b[0m in \u001b[0;36mpredict_batch\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mall_logits\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m                 \u001b[0mall_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m                 all_logits = np.concatenate(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from fast_bert.prediction import BertClassificationPredictor\n",
    "from tqdm import tqdm\n",
    "from Entrainement.CamemBERT.camemBERT import text_prepare\n",
    "from config import CFG\n",
    "import pandas as pd\n",
    "prediction = []\n",
    "DATA_PATH = os.path.join(CFG.path_bert,'Data/')\n",
    "MODEL_PATH = os.path.join(CFG.path_models,'CamemBERT',  'CamemBERT_v3')\n",
    "predictor = BertClassificationPredictor(\n",
    "    model_path=MODEL_PATH,\n",
    "    label_path=DATA_PATH,  # location for labels.csv file\n",
    "    multi_label=True,\n",
    "    model_type='camembert-base',\n",
    "    do_lower_case=False,\n",
    "    device=None)\n",
    "\n",
    "df_test = pd.read_csv('/home/jeremy/Documents/GitHub/auchan_cdl/Entrainement/CamemBERT/Data/test.csv', index_col=False)\n",
    "texts = df_test.text.tolist()\n",
    "texts = [text_prepare(t) for t in texts]\n",
    "for text in tqdm(texts):\n",
    "    prediction.append(predictor.predict(text))\n",
    "preds = [p[0][0] for p in prediction]\n",
    "scores = [p[0][1] for p in prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_test['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "colonnes = df_test.columns.tolist()\n",
    "true_labels = [colonnes[df_test.iloc[i].values.tolist().index(1.0)] for i in range(len(df_test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_ratio(pred,true_labels):\n",
    "    vrai=0\n",
    "    tot = len(pred)\n",
    "    for p, l in zip(pred,true_labels):\n",
    "        if p==l:\n",
    "            vrai+=1\n",
    "    return vrai/tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7651006711409396"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_ratio(preds, true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
>>>>>>> Stashed changes
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3d6b91a3955ec9054229600ec863c492cc972c6f97d69df1e6e0d1a8c674dcee"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('pytorch-awesome': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
